<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

  <meta name="description" content="Holden Lee's Blog">
  <meta name="author" content="Holden Lee">
    
  <title>Research</title>

  <!-- Bootstrap core CSS -->
  <link href="./bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
  <!-- <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

  <!-- Custom styles for this template -->
  <link href="./css/blog.css" rel="stylesheet">
  <link href="./css/default.css" rel="stylesheet">

  <!-- Extension : Footnotes -->
  <link href="./footnotes/css/footnotes.css" rel="stylesheet">

  <!-- Sidebar 
  <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
  -->

  <!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->
  

  
  <!-- Extension : Share buttons @ http://www.sharethis.com/get-sharing-tools/# -->
  <script type="text/javascript" src="http://w.sharethis.com/button/buttons.js">  </script>
  <script type="text/javascript">stLight.options({publisher: "307929ab-483f-4a25-8ef0-76ff5bd12e05", doNotHash: false, doNotCopy: false, hashAddressBar: false});</script>
  

</head>

<body>

<!-- Navigation bar. navbar-inverse is black, navbar-default is white.-->
<!-- To make a button active (pressed), use <li class="active"> -->
<div id="header">
  <nav class="navbar navbar-inverse navbar-static-top"> <!--navbar-fixed-top-->
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="./">Home</a>
      </div>
      <div id="navbar" class="collapse navbar-collapse">
        <ul class="nav navbar-nav">
          <li><a href="./sitemap.html">Sitemap</a></li>
	  <li><a href="./About.html">About</a></li>
          <li><a href="./Math.html">Math</a></li>
	  <li><a href="./Research.html">[Research]</a></li>
	  <li><a href="./Math notes.html">[Math notes]</a></li>
	  <li><a href="./Writing.html">Writing</a></li>
	  <li><a href="./Creations.html">[Creations]</a></li>
	  <li><a href="./Education.html">Education</a></li>
	  <li><a href="./Views.html">Views</a></li>
	  <li><a href="./Projects.html">Projects</a></li>
	  <li><a href="./Topics.html">Topics</a></li>
	  <li><a href="./Links.html">Links</a></li>
<!-- TODO: make this part a for loop over main pages -->
        </ul>
      </div><!--/.nav-collapse -->
    </div>
  </nav>
</div>

<!--
<div class="w3-sidebar w3-light-grey w3-bar-block" style="width:25%">
  <h3 class="w3-bar-item">Menu</h3>
  <a href="#" class="w3-bar-item w3-button">Link 1</a>
  <a href="#" class="w3-bar-item w3-button">Link 2</a>
  <a href="#" class="w3-bar-item w3-button">Link 3</a>
</div>
-->


<!-- Content -->


<div class="container">

  <div id="content">
    <div class="page header">
      <h1> Research </h1>
    </div>
    
    
    <div class="info">
      
       
        <p>Posted: 2017-08-07 
          , Modified: 2022-11-27 
	</p>
      
       <p>Tags: <a href="./tags/math.html">math</a></p> 
       <p>Parent: <a href="./Math.html">Math</a></p> 
	   <p>Children: <a href="./Online%20Sampling%20from%20Log-Concave%20Distributions.html">Online Sampling from Log-Concave Distributions</a>, <a href="./Simulated%20tempering%20Langevin%20Monte%20Carlo.html">Simulated tempering Langevin Monte Carlo</a>, <a href="./l-adic%20properties%20of%20partition%20functions.html">l-adic properties of partition functions</a></p> 
    </div>
    
    
  </div>
  <!--/div-->

  <div class="toc"> <ul>
 <li><a href="#research-interests">Research interests</a></li>
 <li><a href="#papers">Papers</a><ul>
 <li><a href="#generative-modeling-learning-probability-distributions">Generative modeling, learning probability distributions</a></li>
 <li><a href="#algorithms-for-sampling-and-counting">Algorithms for sampling and counting</a></li>
 <li><a href="#reinforcement-learning-and-control-theory">Reinforcement learning and control theory</a></li>
 <li><a href="#natural-language-processing">Natural language processing</a></li>
 <li><a href="#neural-networks">Neural networks</a></li>
 <li><a href="#machine-learning-other">Machine Learning (other)</a></li>
 <li><a href="#complexity-theory">Complexity theory</a></li>
 <li><a href="#number-theory">Number theory</a></li>
 </ul></li>
 <li><a href="#survey-slides">Survey slides</a></li>
 </ul> </div>

  <div class="blog-main">
    <p>I received my Ph.D. from Princeton, where I was advised by <a href="https://www.cs.princeton.edu/~arora/">Sanjeev Arora</a>.</p>
<p>I focus on machine learning theory and applied probability, and also have broad interests in theoretical computer science and related math.</p>
<h2 id="research-interests">Research interests</h2>
<!-- Although machine learning (and deep learning in particular) has made great advances in recent years, our mathematical understanding of it is shallow. Learning problems can be highly nonconvex, yet tractable in practice. What hidden structure do these problems have, and how can we design algorithms to take advantage of it? -->
<p>Current interests include:</p>
<ul>
<li><strong>Probabilistic methods in machine learning</strong>, with a focus on <strong>sampling (MCMC) algorithms</strong>: How to design provable algorithms for learning probability distributions and sampling from them? <!--How can we improve classical algorithms like Markov Chain Monte Carlo, or test the quality of the samples?--></li>
<li><strong>Control theory and reinforcement learning</strong>, with a focus on <strong>learning dynamical systems</strong>: It is a well-studied problem how to find the optimal control for a known linear dynamical system. However, reinforcement learning deals with learning how to act in unknown, combinatorially complex systems; algorithms are heuristic and slow. How to bridge this gap?</li>
<li><strong>Neural networks</strong>: Neural networks tackle highly nonconvex problems but do very well in practice. Why? What kind of algorithmic improvements can we come up with by understanding their theoretical foundations more deeply?</li>
<li><strong>Natural language understanding</strong>: Language is a fundamental part of human intelligence and a big frontier for machine learning. How do we create machines that can understand “grammar” and “semantics”?</li>
</ul>
<h2 id="papers">Papers</h2>
<p>The publication list is available as <a href="https://www.dropbox.com/s/7fas5lckj99sqx0/publication_list.pdf?dl=0">pdf</a>.</p>
<p>[A] denotes alphabetical order of authors.</p>
<!-- Pitfalls of Gaussians as a noise distribution in NCE
Holden Lee, Chirag Pabbaraju, Anish Sevekari, Andrej Risteski
[arxiv](https://arxiv.org/abs/2210.00189)
-->
<h3 id="generative-modeling-learning-probability-distributions">Generative modeling, learning probability distributions</h3>
<ul>
<li><p><strong>Provable benefits of score matching</strong></p>
<p>Chirag Pabbaraju, Dhruv Rohatgi, Anish Prasad Sevekari, Holden Lee, Ankur Moitra, Andrej Risteski.</p>
<p><a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/c11f8d40c119867e30e3421f696f931d-Abstract-Conference.html">NeurIPS 2023</a> (Spotlight). [<a href="https://arxiv.org/abs/2306.01993">arxiv</a>]</p></li>
<li><p><strong>The probability flow ODE is provably fast</strong></p>
<p>[A] Sitan Chen, Sinho Chewi, <strong>Holden Lee</strong>, Yuanzhi Li, Jianfeng Lu, Adil Salim</p>
<p>NeurIPS 2023. [<a href="https://arxiv.org/abs/2306.01993">arxiv</a>)</p></li>
<li><p><strong>Improved Analysis of Score-based Generative Modeling: User-Friendly Bounds under Minimal Smoothness Assumptions</strong></p>
<p>[A] Hongrui Chen, <strong>Holden Lee</strong>, and Jianfeng Lu.</p>
<p>ICML 2023. [<a href="https://arxiv.org/abs/2211.01916">arxiv</a>]</p></li>
<li><p><strong>Pitfalls of Gaussians as a noise distribution in NCE</strong></p>
<p><strong>Holden Lee</strong>, Chirag Pabbaraju, Anish Sevekari, and Andrej Risteski.</p>
<p>ICLR 2023, NeurIPS 2022 Workshop on Self-Supervised Learning. [<a href="https://arxiv.org/abs/2210.00189">arxiv</a>]</p></li>
<li><p><strong>Convergence of score-based generative modeling for general data distributions</strong></p>
<p>[A] <strong>Holden Lee</strong>, Jianfeng Lu, and Yixin Tan.</p>
<p>NeurIPS 2022 Workshop on Score-Based Methods. [<a href="https://arxiv.org/abs/2209.12381">arxiv</a>]</p></li>
<li><p><strong>Convergence for score-based generative modeling with polynomial complexity</strong></p>
<p>[A] <strong>Holden Lee</strong>, Jianfeng Lu, and Yixin Tan.</p>
<p><a href="https://neurips.cc/virtual/2022/poster/53863">NeurIPS 2022</a> (oral). [<a href="https://arxiv.org/abs/2206.06227">arxiv</a>, <a href="https://www.dropbox.com/s/una0wbbr4jknz0l/score-talk.pdf?dl=0">slides</a>]</p></li>
<li><p><strong>Universal Approximation for Log-concave Distributions using Well-conditioned Normalizing Flows.</strong></p>
<p><strong>Holden Lee</strong>, Chirag Pabbaraju, Anish Sevekari, Andrej Risteski.</p>
<p>NeurIPS 2021. [<a href="https://arxiv.org/abs/2107.02951">arXiv</a>, <a href="https://arxiv.org/pdf/2107.02951">pdf</a>, <a href="https://www.dropbox.com/s/r0jp53sqqaol4bj/NF%20-%20CMU.pptx?dl=0">slides</a>]</p></li>
</ul>
<h3 id="algorithms-for-sampling-and-counting">Algorithms for sampling and counting</h3>
<ul>
<li><p><strong>Sampling List Packings</strong></p>
<p>[A] Evan Camrud, Ewan Davies, Alex Karduna, Holden Lee.</p>
<p><a href="https://arxiv.org/abs/2402.03520">arxiv</a></p></li>
<li><p><strong>Parallelising Glauber Dynamics</strong></p>
<p>Holden Lee.</p>
<p><a href="https://arxiv.org/abs/2307.07131">arxiv</a></p></li>
<li><p><strong>Fisher information lower bounds for sampling</strong></p>
<p>[A] Sinho Chewi, Patrik Gerber, <strong>Holden Lee</strong>, Chen Lu.</p>
<p>In submission. [<a href="https://arxiv.org/abs/2210.02482">arxiv</a>]</p></li>
<li><p><strong>Sampling Approximately Low-Rank Ising Models: MCMC meets Variational Methods</strong></p>
<p>[A] Frederic Koehler, <strong>Holden Lee</strong>, and Andrej Risteski.</p>
<p>COLT 2022. [<a href="https://arxiv.org/abs/2202.08907">arXiv</a>, <a href="https://arxiv.org/pdf/2202.08907">pdf</a>, <a href="https://www.dropbox.com/s/grd6fo5aa640kra/lri-talk.pdf?dl=0">slides</a>, <a href="https://slideslive.com/38985703/sampling-approximately-lowrank-ising-models-mcmc-meets-variational-methods?ref=folder-104261">video</a>]</p></li>
<li><p><strong>Approximation algorithms for the random-field Ising model</strong></p>
<p>[A] Tyler Helmuth, <strong>Holden Lee</strong>, Will Perkins, Mohan Ravichandran, and Qiang Wu.</p>
<p>SIAM Journal on Discrete Mathematics 37 (3), 1610-1629. [<a href="https://arxiv.org/abs/2108.11889">arXiv</a>, <a href="https://arxiv.org/pdf/2108.11889">pdf</a>]</p></li>
<li><p><strong>Efficient sampling from the Bingham distribution</strong></p>
<p>[A] Rong Ge, <strong>Holden Lee</strong>, Jianfeng Lu, and Andrej Risteski.</p>
<p>ALT 2021. [<a href="https://arxiv.org/abs/2010.00137">arXiv</a>, <a href="https://arxiv.org/pdf/2010.00137">pdf</a>, <a href="https://www.youtube.com/watch?v=6dkS-Hdg8cs">video</a>]</p></li>
<li><p><strong>Estimating Normalizing Constants for Log-Concave Distributions: Algorithms and Lower Bounds</strong></p>
<p>[A] Rong Ge, <strong>Holden Lee</strong>, and Jianfeng Lu.</p>
<p>STOC 2020. [<a href="https://arxiv.org/abs/1911.03043">arXiv</a>, <a href="https://arxiv.org/pdf/1911.03043">pdf</a>, <a href="https://dl.acm.org/citation.cfm?id=3384289">STOC 2020:579–586</a>, <a href="https://www.dropbox.com/s/da2mz50a76k9suz/NormalizingConstant_Presentation.pdf?dl=0">slides</a>, <a href="https://www.youtube.com/watch?v=TqdtxcYaw3I&amp;list=PLn0nrSd4xjjadfcMd5xvmJ_GNSLDi1ATn">video</a>]</p></li>
<li><p><strong>Online Sampling from Log-Concave Distributions</strong></p>
<p>[A] <strong>Holden Lee</strong>, Oren Mangoubi, and Nisheeth Vishnoi.</p>
<p>NeurIPS 2019. [<a href="https://arxiv.org/abs/1902.08179">arXiv</a>, <a href="https://arxiv.org/pdf/1902.08179">pdf</a>, <a href="Online%20Sampling%20from%20Log-Concave%20Distributions.html">webpage</a>]</p></li>
<li><p><strong>Beyond Log-concavity: Provable Guarantees for Sampling Multi-modal Distributions using Simulated Tempering Langevin Monte Carlo.</strong> <a href="Simulated%20tempering%20Langevin%20Monte%20Carlo.html">webpage</a></p>
<p>[A] Rong Ge, <strong>Holden Lee</strong>, and Andrej Risteski.</p>
<ul>
<li>NeurIPS 2018. [<a href="https://arxiv.org/abs/1812.00793">arXiv</a>, <a href="https://arxiv.org/pdf/1812.00793.pdf">pdf</a>]</li>
<li>NIPS AABI Workshop 2017. [<a href="https://arxiv.org/abs/1710.02736">arXiv</a>, <a href="https://arxiv.org/pdf/1710.02736.pdf">pdf</a>]</li>
<li>Blog post on offconvex: <a href="http://www.offconvex.org/2020/09/19/beyondlogconvavesampling/">1</a>, <a href="http://www.offconvex.org/2021/03/01/beyondlogconcave2/">2</a>.</li>
</ul></li>
</ul>
<h3 id="reinforcement-learning-and-control-theory">Reinforcement learning and control theory</h3>
<ul>
<li><p><strong>Extracting Latent State Representations with Linear Dynamics from Rich Observations</strong></p>
<p>Abraham Frandsen, Rong Ge, and <strong>Holden Lee</strong>.</p>
<p><a href="https://proceedings.mlr.press/v162/frandsen22a.html">ICML 2022</a>.</p></li>
<li><p><strong>Improved rates for identification of partially observed linear dynamical systems</strong></p>
<p><strong>Holden Lee</strong>.</p>
<p>ALT 2022. [<a href="https://arxiv.org/abs/2011.10006">arXiv</a>, <a href="https://arxiv.org/pdf/2011.10006">pdf</a>]</p></li>
<li><p><strong>No-Regret Prediction in Marginally Stable Systems</strong></p>
<p>[A] Udaya Ghai, <strong>Holden Lee</strong>, Karan Singh, Cyril Zhang, and Yi Zhang.</p>
<p>COLT 2020. [<a href="https://arxiv.org/abs/2002.02064">arxiv</a>, <a href="https://arxiv.org/pdf/2002.02064.pdf">pdf</a>, <a href="https://www.dropbox.com/s/74idgddlkbosiil/slides-colt.pdf?dl=0">slides</a>, <a href="https://www.dropbox.com/s/rbmr4rk3vixahoc/slides-colt-short.pdf?dl=0">summary slide</a>, <a href="https://www.colt2020.org/virtual/papers/paper_34.html">videos</a>]</p></li>
<li><p><strong>Statistical Guarantees for Learning an Autoregressive Filter</strong></p>
<p>[A] <strong>Holden Lee</strong> and Cyril Zhang.</p>
<p>ALT 2020. [<a href="https://arxiv.org/abs/1905.09897">arxiv</a>, <a href="https://arxiv.org/pdf/1905.09897.pdf">pdf</a>]</p></li>
<li><p><strong>Spectral Filtering for General Linear Dynamical Systems</strong></p>
<p>[A] Elad Hazan, <strong>Holden Lee</strong>, Karan Singh, Cyril Zhang, and Yi Zhang.</p>
<p>NeurIPS 2018 (oral). [<a href="https://arxiv.org/abs/1802.03981">arxiv</a>, <a href="https://arxiv.org/pdf/1802.03981.pdf">pdf</a>]</p></li>
<li><p><strong>Towards Provable Control for Unknown Linear Dynamical Systems.</strong></p>
<p>[A] Sanjeev Arora, Elad Hazan, <strong>Holden Lee</strong>, Karan Singh, Cyril Zhang, and Yi Zhang.</p>
<p>ICLR workshop 2018. [<a href="https://openreview.net/forum?id=HJGuXK1vM">ICLR page</a>, <a href="https://openreview.net/pdf?id=HJGuXK1vM">pdf</a>]</p></li>
</ul>
<h3 id="natural-language-processing">Natural language processing</h3>
<ul>
<li><p><strong>Principled Gradient-based Markov Chain Monte Carlo for Text Generation</strong></p>
<p>Li Du, Afra Amini, Lucas Torroba Hennigen, Xinyan Velocity Yu, Jason Eisner, Holden Lee, Ryan Cotterell.</p>
<p><a href="https://arxiv.org/abs/2312.17710">arxiv</a></p></li>
<li><p><strong>Connecting Pre-trained Language Model and Downstream Task via Properties of Representation</strong></p>
<p>Chenwei Wu, Holden Lee, Rong Ge.</p>
<p><a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/93712c59f6a81bd92040facf04c8b308-Abstract-Conference.html">NeurIPS 2023</a>.</p></li>
</ul>
<h3 id="neural-networks">Neural networks</h3>
<ul>
<li><p><strong>Explaining Landscape Connectivity of Low-cost Solutions for Multilayer Nets.</strong></p>
<p>Rohith Kuditipudi, Xiang Wang, <strong>Holden Lee</strong>, Yi Zhang, Zhiyuan Li, Wei Hu, Rong Ge, and Sanjeev Arora.</p>
<p>NeurIPS 2019. [<a href="https://arxiv.org/abs/1906.06247">arXiv</a>, <a href="https://arxiv.org/pdf/1906.06247">pdf</a>]</p></li>
<li><p><strong>On the Ability of Neural Nets to Express Distributions.</strong></p>
<p><strong>Holden Lee</strong>, Rong Ge, Tengyu Ma, Andrej Risteski, and Sanjeev Arora.</p>
<p>COLT 2017. [<a href="https://arxiv.org/abs/1702.07028">arXiv</a>, <a href="https://arxiv.org/pdf/1702.07028.pdf">pdf</a>, <a href="http://proceedings.mlr.press/v65/lee17a/lee17a.pdf">PMLR 65:1271-1296</a>, <a href="http://tiny.cc/hlcolt17">webpage</a>]</p></li>
</ul>
<h3 id="machine-learning-other">Machine Learning (other)</h3>
<ul>
<li><p><strong>How Flawed is ECE? An Analysis via Logit Smoothing</strong></p>
<p>[A] Muthu Chidambaram, Holden Lee, Colin McSwiggen, Semon Rezchikov.</p>
<p><a href="https://arxiv.org/abs/2402.10046">arxiv</a></p></li>
</ul>
<h3 id="complexity-theory">Complexity theory</h3>
<ul>
<li><p><strong>Quadratic polynomials of small modulus cannot represent OR.</strong></p>
<p><strong>Holden Lee</strong></p>
<p>Unpublished, 2015. [<a href="http://arxiv.org/abs/1509.08896">arXiv</a>, <a href="http://arxiv.org/pdf/1509.08896.pdf">pdf</a>]</p></li>
</ul>
<h3 id="number-theory">Number theory</h3>
<ul>
<li><p><strong>l-adic properties of partition functions.</strong></p>
<p>[A] Eva Belmont, <strong>Holden Lee</strong>, Alexandra Musat, and Sarah Trebat-Leder.</p>
<p><a href="http://link.springer.com/article/10.1007/s00605-013-0586-y">Monatshefte für Mathematik, 173(1), 1-34</a>, 2014. [<a href="https://arxiv.org/abs/1510.01202">arXiv</a>, <a href="https://arxiv.org/pdf/1510.01202.pdf">pdf</a>, <a href="https://www.dropbox.com/s/81413cszqabcwcx/MIT%20presentation.pdf?dl=0">presentation</a>, <a href="l-adic%20properties%20of%20partition%20functions.html">webpage</a>]</p></li>
</ul>
<h2 id="survey-slides">Survey slides</h2>
<ul>
<li><a href="https://www.dropbox.com/s/uuc977hxlu1ipe6/probabilistic-foundations-jhu.pdf?dl=0">Probabilistic foundations for machine learning</a> (Job talk, 2022)</li>
<li><a href="https://www.dropbox.com/s/gy8huf140rkt414/changing-the-temperature.pdf?dl=0">Changing the temperature for algorithm design</a> (Frontiers of Statistical Mechanics and Theoretical Computer Science, 2021/12/14)</li>
</ul>

  </div>
  
    

    <!-- Extension : Sharing buttons @ www.sharethis.com -->
    <span class="st_facebook_large" displayText="Facebook"></span>
    <span class="st_twitter_large" displayText="Tweet"></span>
    <span class="st_googleplus_large" displayText="Google +"></span>
    <span class="st_reddit_large" displayText="Reddit"></span>
    <span class="st__large" displayText></span>

    <div id="disqus_thread"></div>
    


  
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
  </div>
  
  
</div>



<!-- Footer -->
<div id="footer">
  <div class="container">
    Built with
    <a href="http://jaspervdj.be/hakyll">Hakyll</a> 
    using 
    <a href="http://www.getbootstrap.com">Bootstrap</a>, 
    <a href="http://www.disqus.com">Disqus</a>,
    <a href="http://ignorethecode.net/blog/2010/04/20/footnotes/">Footnotes.js</a>,
    <a href="http://highlightjs.org/">Highlight.js</a>, 
    <a href="http://www.mathjax.org">MathJax</a>, 
    and <a href="http://www.sharethis.com">ShareThis</a>.
  </div>
</div>
</body>

</html>

<!-- SCRIPTS -->
<!-- jQuery-->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

<script src="./bootstrap/js/bootstrap.min.js"></script>

<!-- Extension : Highlight.js @ https://highlightjs.org/ -->
<!-- Syntax highlighting tomorrow-night-bright, agate-->
<link rel="stylesheet" href="./highlight/css/tomorrow-night-bright.css">
<script src="./highlight/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<!-- Extension : MathJax @ https://docs.mathjax.org/en/v2.5-latest/tex.html -->
<!-- MathJax/config/local/local.js contains macros. Need to provide entire URL-->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML,http://holdenlee.github.io/MathJax/config/local/local"></script>

<!-- Extension : Footnotes @ http://ignorethecode.net/blog/2010/04/20/footnotes/ -->
<script src="./footnotes/js/footnotes.js"></script>

<!-- Extension : Disqus @ http://disqus.com -->
<!-- Extension : InlineDisqussions @ https://github.com/tsi/inlineDisqussions -->

<script src="./disqus/js/disqus.js"></script>



<!-- Extension : Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-73261814-1', 'auto');
  ga('send', 'pageview');

</script>

